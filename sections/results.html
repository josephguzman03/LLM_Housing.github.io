<div class="section-content">
  <link rel="stylesheet" href="static/css/common.css">

    <h3>Exploratory Data Analysis</h3>
    <p> 
      We first cleaned and standardized the data, extracting numerical scores using regex and ensuring consistency in formatting. If multiple scores appeared in a response, we averaged them to maintain a single value per prompt.
    </p>
    <p>
    Out of 285,120 prompts, 13.27% (37,842) were classified as refusals, where models either declined to respond or returned an incorrectly formatted score. Most models had very low refusal rates, with OpenAI and Meta’s models refusing fewer than 10 prompts each. Google’s gemma-2-2b-it had a 0.5% refusal rate, while InceptionAI’s jais-family-1p3b-chat stood out with a refusal rate of 79.11%, likely due to difficulties following the prompt format.
      </p>
      <p>
        Score distribution ranged from 0 to 100, with an overall mean of 75.4, suggesting generally favorable responses. However, model-specific variations in performance required further analysis, detailed in the next sections.
      </p>
      <br>
      <h3>Prompt 1 Results </h3>
      <br>
      <h4>Differences by Race</h4>
      <img src="static/images/p1plots/distribution_of_responses_for_each_model_model_race.png" alt="dist_img">

    
      <h4>Differences by Gender</h4>
      <p> 
        To start off, our analysis of gender-based score differences showed that while median scores remained consistent (typically 80–85), the spread and presence of outliers varied across models. OpenAI’s models produced stable scores across gender groups, with mean values between 77 and 83. In contrast, Google’s gemma-2-2b-it had a lower median score of 65. InceptionAI’s jais-family-1p3b-chat and Meta’s Meta-Llama-3-8B-Instruct exhibited broader distributions, spanning 0 to 100. Most models showed left-skewed distributions, indicating a higher frequency of high scores.      </p>
      <img src="static/images/p1plots/distribution_of_responses_for_each_model_model_gender.png" alt="dist_img">

      <p>
        We used the Kruskal-Wallis test since the models didn’t meet parametric assumptions. Significant differences were found in multiple models: Google’s gemma-2-2b-it showed differences across all gender groups, while OpenAI’s and Meta’s models varied mostly between gender-neutral and woman groups. Despite these findings, the largest mean gap was just 1.41 points, making the differences statistically significant but practically negligible.
        </p>

      <h4>Differences by Occupation</h4>
      <p>
        Occupational labels had a stronger impact on model scores than gender or race. OpenAI’s models and Google’s gemma-2-2b-it showed more consistent scores, while InceptionAI’s jais-family-1p3b-chat and Meta’s Meta-Llama-3-8B-Instruct had wider score ranges with more outliers. InceptionAI and Meta’s models showed greater score disparities, while OpenAI’s and Google’s models followed more stable patterns. These results suggest that some models are more sensitive to occupational labels than others.
      </p>
      <img src="static/images/p1plots/distribution_of_responses_for_each_model_model_occupation.png" alt="dist_img">

      <p>
        Models assigned higher scores to doctors, software engineers, and teachers, while unemployed individuals received the lowest. Google’s gemma-2-2b-it and OpenAI’s models showed the most variation, with score gaps between doctors and unemployed individuals ranging from 26.76 to 43.78 points—far greater than differences for gender or race. InceptionAI’s model showed minimal variation but had the highest refusal rate (79.11%). These results suggest occupational biases in AI models, which could reinforce socioeconomic inequalities in automated decision-making.
      </p>
      <h4>Differences by Living Status</h4>
      <p>
        No single living status category consistently received the highest scores across models. OpenAI’s models assigned similar scores across groups, while Google’s gemma-2-2b-it and Meta’s Meta-Llama-3-8B-Instruct showed more variation, with individuals living alone or with a spouse tending to score higher.
      </p>

      <img src="static/images/p1plots/distribution_of_responses_for_each_model_model_living_status.png" alt="dist_img">

      <p>
        Statistical tests confirmed significant differences in some models, with Google’s gemma-2-2b-it showing the most variation (13 out of 15 significant comparisons) and the largest mean difference of 5.36. Meta’s model followed closely, while OpenAI’s models exhibited smaller differences, with maximum mean gaps below 1.6. InceptionAI’s jais-family-1p3b-chat showed no significant differences, indicating uniform treatment of living status.
      </p>
      <p>
        Overall, while some models distinguished between living status groups, these differences were relatively small, especially compared to occupation-related disparities.
      </p>
      <h4>Differences by Several Variables</h4>

      <h3>Prompt 2 Results </h3>

      <h4>Differences by Race</h4> 
      <img src="static/images/p2plots/distribution_of_responses_for_each_model_model_race.png" alt="dist_img">


      <h4>Differences by Gender</h4>
      <img src="static/images/p2plots/distribution_of_responses_for_each_model_model_gender.png" alt="dist_img">

      <h4>Differences by Credit Score </h4>
      <img src="static/images/p2plots/distribution_of_responses_for_each_model_model_credit_scores.png" alt="dist_img">

  
      <h4>Differences by Eviction Status</h4>
      <img src="static/images/p2plots/distribution_of_responses_for_each_model_model_eviction.png" alt="dist_img">

      <h3>Conclusion </h3>



  </div>