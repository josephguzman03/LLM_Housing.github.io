<div class="section-content">
    <h2>Intro</h2>
    <p>
        The U.S. housing crisis has made securing housing highly competitive, while Large Language Models (LLMs) like ChatGPT are increasingly used for decision-making. However, these models, trained on biased historical data, may reinforce societal biases, potentially impacting critical decisions such as tenant selection and housing eligibility.  </p>
  
  <p>
    Our project examines how LLM biases manifest in housing-related contexts. We generate prompts reflecting real-life housing decisions and analyze LLM responses for discrepancies, focusing on affected groups and broader societal impacts.  </p>
  
  <p>
    Building on algorithm audit research, we assess biases in LLM-generated outputs related to housing, including tenant screening and eviction risk. We introduce variations in personal details like gender and race to study biased responses. Statistical methods, including ANOVA, t-tests, and Kruskal-Wallis tests, are applied to detect significant disparities, with results visualized through heat maps and boxplots.  </p>
  </div>