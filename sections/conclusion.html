<div class="section-content fade-in">
    <h2>Conclusion</h2>
    <p>
      Our audit examined how large language models (LLMs) generate tenant scores based on application data, revealing that algorithmic decision-making can inadvertently reinforce existing societal inequities. By comparing multiple models across two different prompt formats, we observed that demographic factors such as gender, race, occupation, living status, eviction history, and credit score can significantly influence scoring outcomes.
    </p>
    <ul>
      <li>
        <strong>Occupation Impact:</strong> Applicants in certain professions (e.g., doctors) received substantially higher scores than unemployed individuals.
      </li>
      <li>
        <strong>Subtle Demographic Biases:</strong> Gender and race differences, though statistically significant, typically accounted for smaller score variations.
      </li>
      <li>
        <strong>Intersectional Effects:</strong> When factors like race and occupation intersect, the biases became more pronounced.
      </li>
      <li>
        <strong>Eviction and Credit Sensitivity:</strong> Models penalized applicants with prior evictions or lower credit scores, sometimes even when the eviction was historical or non-indicative of current risk.
      </li>
    </ul>
    <p>
      While these results contribute valuable insights into LLM bias, the study is limited by its focus on a single geographic region (San Diego) and simplified application formats. Future research should aim to:
    </p>
    <ul>
      <li>
        <strong>Broaden Scope:</strong> Extend analyses to diverse geographic regions and a wider range of housing scenarios.
      </li>
      <li>
        <strong>Enhance Prompt Complexity:</strong> Incorporate varied communication styles and more comprehensive applicant details.
      </li>
      <li>
        <strong>Benchmark Against Human Decisions:</strong> Compare LLM-based scoring with human decision-making to better understand real-world impacts.
      </li>
      <li>
        <strong>Develop Mitigation Strategies:</strong> Explore ways to reduce biases in AI-driven tenant screening to foster fairer housing practices.
      </li>
    </ul>
    <p>
      Our findings underscore the importance of ongoing research and informed policy-making to mitigate the discriminatory impacts of AI in housing, ensuring that technological advancements promote rather than hinder equity.
    </p>
  </section>
</div>