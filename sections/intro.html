<div class="section-content">
    <h2>Intro</h2>
    <p>
      In recent years, the housing crisis has impacted millions across the United States, driven by rising living costs and increasing housing demand. As a result, the housing application process has become highly competitive, with many units receiving dozens of applicants. Simultaneously, Large Language Models (LLMs) like ChatGPT have become popular tools for decision-making, used by both individuals and businesses. However, because LLMs are trained on historical data that often reflects societal biases related to race, gender, and income, their responses can inadvertently perpetuate these biases, negatively influencing critical decisions—particularly in life-changing areas like housing. It is crucial to recognize and understand the implications of relying on these technologies in such contexts.
  </p>
  
  <p>
      Our project seeks to explore how these biases manifest in LLM-generated responses, specifically in the context of the housing crisis—a critical issue driving many current policies. We aim to create housing-related prompts from the perspective of ordinary individuals who rely on LLM feedback for decisions such as identifying suitable housing options, determining eligibility for programs, or making tenant selection choices as landlords. These prompts will be crafted based on personal insights and public feedback, collected through interviews, to ensure relevance and inclusivity. By analyzing the responses generated by LLMs to these prompts, we will investigate potential discrepancies and biases, with a focus on identifying the groups most affected and understanding how these biases affect outcomes. Understanding these discrepancies will help us uncover the mechanisms behind LLM decision-making and provide insights into their broader societal impact, particularly for vulnerable populations.
  </p>
  
  <p>
      Building upon previous research in algorithm audits, our project aims to fill a notable gap in Large Language Models (LLMs) auditing within the housing sector. Utilizing established frameworks and methodologies, we investigate potential biases and discrepancies in LLM outputs related to housing-related prompts, including topics such as housing program eligibility, tenant screening assistance, eviction risk analysis, and housing need scoring. As part of our methodology, we conducted interviews to refine these topics and introduce variations in personal information like gender and race in our prompts to understand factors that influence biased outputs. The culmination of our work emphasizes quantitative analysis, using LLM responses as primary data. Assumptions for statistical tests will be checked before applying the appropriate methods. If met, parametric tests like ANOVA and t-tests will be used; otherwise, alternatives like the Kruskal-Wallis test will be applied. Significant results will be further analyzed using Dunn’s test, with findings being visualized through heat maps and boxplots.
  </p>
  </div>